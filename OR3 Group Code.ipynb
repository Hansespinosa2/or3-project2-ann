{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72b191b",
   "metadata": {},
   "source": [
    "## Answer 1 – Paola Valdes-Sueiras​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf25fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load training and test data from Excel files\n",
    "X_train = pd.read_csv('train_data_set.csv', header=None).values # Training Data\n",
    "y_train = pd.read_csv('train_label_set.csv', header=None).values.flatten() # Training Labels\n",
    "X_test = pd.read_csv('test_data_set.csv', header=None).values # Test Data\n",
    "y_test = pd.read_csv('test_label_set.csv', header=None).values.flatten() # Test Labels\n",
    "\n",
    "# Parameters\n",
    "m = 20 # Number of activation functions\n",
    "lambda_ = 0.01 # Regularization parameter\n",
    "step_size = 0.001 # Gradient of descent step size\n",
    "num_epochs = 1000 # Number of training epochs\n",
    "\n",
    "# Initialize weights randomly\n",
    "d = X_train.shape[1] # Number of features\n",
    "W = np.random.randn(m, d) # Initialize weights for m activation functions\n",
    "\n",
    "# Linear activation function\n",
    "def F_ANN(X, W):\n",
    "   return np.dot(X, W.T).sum(axis=1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "   # Compute predictions\n",
    "   preds = F_ANN(X_train, W)\n",
    "   # Compute gradient of loss with respect to w\n",
    "   error = preds - y_train\n",
    "   loss_grad = (1 / len(y_train)) * np.dot(error, X_train)\n",
    "   # Add regularization gradient\n",
    "   reg_grad = lambda_ * np.sign(W)\n",
    "   grad = loss_grad + reg_grad\n",
    "   # Gradient descent update\n",
    "   W -= step_size * grad\n",
    "   # Compute and print loss for monitoring\n",
    "   loss = (1 / (2 * len(y_train))) * np.sum(error ** 2) + lambda_*np.sum(np.abs(W))\n",
    "   if epoch % 100 == 0:\n",
    "       print(f\"Epoch {epoch}, Loss: {loss: .4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_preds = F_ANN(X_test, W)\n",
    "test_loss = (1 / (2 * len(y_test))) * np.sum((test_preds - y_test)**2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0eea8",
   "metadata": {},
   "source": [
    "## Answer 2 - Andres ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ec1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from handle_data import load_data\n",
    "import ml as ml\n",
    "import numpy as np\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data('data')\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "model = ml.NeuralNetwork([input_size, 100, 100, 1], ['relu', 'relu', 'relu'])\n",
    "model.train(X_train.T, y_train.reshape(1,-1), epochs=100, cost_fn='l1_reg_mse',reg_lambda=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe9152",
   "metadata": {},
   "source": [
    "## Answer 3 – Andrea Riquezes Gete​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af933b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV files\n",
    "train_data = np.loadtxt(\"train_data_set.csv\", delimiter=\",\")\n",
    "train_labels = np.loadtxt(\"train_label_set.csv\", delimiter=\",\").reshape(-1, 1)\n",
    "test_data = np.loadtxt(\"test_data_set.csv\", delimiter=\",\")\n",
    "test_labels = np.loadtxt(\"test_label_set.csv\", delimiter=\",\").reshape(-1, 1)\n",
    "\n",
    "# Dimensions\n",
    "N, d = train_data.shape  # Number of samples and dimensions\n",
    "m = 20  # Number of neurons (as per problem statement)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_reg = 0.3  # Regularization parameter\n",
    "tol = 1e-6        # Tolerance for convergence\n",
    "max_iters = 100   # Maximum iterations for Newton's method\n",
    "\n",
    "\n",
    "# Initialize weights W randomly\n",
    "W = np.random.rand(d, m)\n",
    "\n",
    "# Define helper functions\n",
    "def compute_loss(X, Y, W, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the objective function value.\n",
    "    \"\"\"\n",
    "    predictions = X @ W @ np.ones((m, 1))  # FAN_N(x, W)\n",
    "    residual = predictions - Y\n",
    "    loss = (1 / (2 * N)) * np.sum(residual**2) + lambda_reg * np.sum(W**2)\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, Y, W, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function.\n",
    "    \"\"\"\n",
    "    predictions = X @ W @ np.ones((m, 1))  # FAN_N(x, W)\n",
    "    residual = predictions - Y\n",
    "    grad = (1 / N) * (X.T @ (residual @ np.ones((1, m)))) + 2 * lambda_reg * W\n",
    "    return grad\n",
    "\n",
    "def compute_hessian(X, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the Hessian of the loss function.\n",
    "    \"\"\"\n",
    "    hessian = (1 / N) * (X.T @ X) + 2 * lambda_reg * np.eye(d)\n",
    "    return hessian\n",
    "\n",
    "alpha = 0.01  # Learning rate multiplier for Newton's step\n",
    "damping_factor = 1e-3  # Damping factor for Hessian\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    # Compute loss, gradient, and Hessian\n",
    "    loss = compute_loss(train_data, train_labels, W, lambda_reg)\n",
    "    grad = compute_gradient(train_data, train_labels, W, lambda_reg)\n",
    "    hessian = compute_hessian(train_data, lambda_reg)\n",
    "\n",
    "    # Dampen Hessian\n",
    "    hessian += damping_factor * np.eye(hessian.shape[0])\n",
    "\n",
    "    # Check gradient norm\n",
    "    grad_norm = np.linalg.norm(grad)\n",
    "    print(f\"Iteration {iteration}: Loss = {loss}, Gradient Norm = {grad_norm}\")\n",
    "\n",
    "    # Solve for the Newton step\n",
    "    try:\n",
    "        hessian_inv = np.linalg.inv(hessian)\n",
    "        step = hessian_inv @ grad\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Hessian is singular; stopping optimization.\")\n",
    "        break\n",
    "\n",
    "    # Update weights with scaled Newton step\n",
    "    W -= alpha * step\n",
    "\n",
    "    # Monitor dynamics\n",
    "    mse_term = (1 / (2 * N)) * np.sum((train_data @ W @ np.ones((m, 1)) - train_labels) ** 2)\n",
    "    reg_term = lambda_reg * np.sum(W**2)\n",
    "    print(f\"Iteration {iteration}: MSE = {mse_term}, Regularization = {reg_term}, Total Loss = {loss}\")\n",
    "\n",
    "    if grad_norm < tol:\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "print(f\"Iteration {iteration}: Gradient Norm = {np.linalg.norm(grad)}\")\n",
    "cond_number = np.linalg.cond(hessian)\n",
    "print(f\"Hessian Condition Number = {cond_number}\")\n",
    "\n",
    "# Compute final predictions\n",
    "final_predictions = train_data @ W @ np.ones((m, 1))\n",
    "\n",
    "# Compute Mean Squared Error term\n",
    "mse_term = (1 / (2 * len(train_labels))) * np.sum((final_predictions - train_labels) ** 2)\n",
    "\n",
    "# Compute Regularization term\n",
    "reg_term = lambda_reg * np.sum(W**2)\n",
    "\n",
    "# Compute Total Loss\n",
    "final_loss = mse_term + reg_term\n",
    "\n",
    "# Output the final loss\n",
    "print(f\"Final Loss: {final_loss}\")\n",
    "print(f\"Mean Squared Error Term: {mse_term}\")\n",
    "print(f\"Regularization Term: {reg_term}\")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = test_data @ W @ np.ones((m, 1))\n",
    "mse_test = np.mean((predictions - test_labels)**2)\n",
    "print(\"Test Set Mean Squared Error:\", mse_test)\n",
    "\n",
    "predictions = train_data @ W @ np.ones((m, 1))\n",
    "print(\"Sample Predictions:\", predictions[:5].flatten())\n",
    "print(\"Sample Labels:\", train_labels[:5].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b515fbf",
   "metadata": {},
   "source": [
    "## Answer 4 - Lindsay Carrillo​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load data\n",
    "train_data = pd.read_csv('/train_data_set.csv')\n",
    "test_data = pd.read_csv('test_data_set.csv')\n",
    "test_labels = pd.read_csv('test_label_set.csv')\n",
    "train_labels = pd.read_csv('train_label_set.csv')\n",
    "\n",
    "\n",
    "X_train = train_data.values\n",
    "y_train = train_labels.values.ravel()\n",
    "X_test = test_data.values\n",
    "y_test = test_labels.values.ravel()\n",
    "\n",
    "# grid search for hyperparameter tuning\n",
    "param_grid = [0.001, 0.01, 0.1, 1, 10, 100]  # possible lambdas\n",
    "#'alpha' used in place of 'lambda' ('lambda is python keyword'):\n",
    "best_alpha = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "for alpha in param_grid:\n",
    "   # train linear model with l2 regularization\n",
    " \n",
    "   # calculate w using normal equation with reg\n",
    "   XTX = X_train.T @ X_train\n",
    "   regularization_term = alpha * np.eye(X_train.shape[1])\n",
    "   XTy = X_train.T @ y_train\n",
    "   w = np.linalg.inv(XTX + regularization_term) @ XTy\n",
    "   \n",
    "   # make predictions on training set\n",
    "   y_pred_train = X_train @ w\n",
    "   \n",
    "   # calculate mean squared error on training set\n",
    "   mse = mean_squared_error(y_train, y_pred_train)\n",
    "   print(f'Lambda: {alpha}, Training Mean Squared Error: {mse}')\n",
    "   # update best alpha if current mse is lower\n",
    "   if mse < best_mse:\n",
    "       best_mse = mse\n",
    "       best_alpha = alpha\n",
    "       best_w = w\n",
    "\n",
    "# make predictions on test set using best model\n",
    "y_pred_test = X_test @ best_w\n",
    "\n",
    "# calc mse on test set\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f'Best lambda: {best_alpha}')\n",
    "print(f'Test Mean Squared Error: {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa924e",
   "metadata": {},
   "source": [
    "## Answer 5 – Yousef Bani Ahmad​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "training_data = sio.loadmat('training_data.mat')\n",
    "train_data, train_label = training_data['train_data_set'], training_data['train_label_set'].ravel()\n",
    "train_label = train_label.reshape(-1, 1)\n",
    "\n",
    "d = train_data.shape[1]\n",
    "m = 20\n",
    "lambda_reg = 0.01\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize weights\n",
    "W = np.random.randn(m, d) * 0.01\n",
    "\n",
    "def activation(x):\n",
    "    return x\n",
    "\n",
    "def model(x, W):\n",
    "    return np.dot(W.T, activation(x))\n",
    "\n",
    "def compute_loss(x, y, W):\n",
    "    predictions = np.dot(x, W.T).sum(axis=1)\n",
    "    errors = predictions - y.squeeze()\n",
    "    loss = (1 / (2 * len(y))) * np.sum(errors ** 2)\n",
    "    reg_term = lambda_reg * np.sum(np.abs(W))\n",
    "    return loss + reg_term\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_data)):\n",
    "        x_i = train_data[i, :].reshape(-1, 1)\n",
    "        y_i = train_label[i]\n",
    "        \n",
    "        # Compute prediction separately for each sample\n",
    "        prediction = np.dot(W, x_i).sum()\n",
    "        \n",
    "        # Gradient of the loss with respect to W\n",
    "        grad_w = np.zeros(W.shape)\n",
    "        \n",
    "        for j in range(m):\n",
    "            grad_w[j] = (1 / len(train_data)) * (prediction - y_i) * x_i.squeeze()\n",
    "        \n",
    "        # Update weights with SGD\n",
    "        W -= learning_rate * (grad_w + lambda_reg * np.sign(W))\n",
    "    \n",
    "    # Print loss for every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        current_loss = compute_loss(train_data, train_label, W)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss}')\n",
    "\n",
    "# Testing the trained model\n",
    "testing_data = sio.loadmat('testing_data.mat')\n",
    "test_data, test_label = testing_data['test_data_set'], testing_data['test_label_set'].ravel()\n",
    "\n",
    "# Predictions for the test dataset\n",
    "test_predictions = np.dot(test_data, W.T).sum(axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(test_label, test_predictions)\n",
    "mae = mean_absolute_error(test_label, test_predictions)\n",
    "r2 = r2_score(test_label, test_predictions)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'R-Squared (R^2): {r2}')\n",
    "\n",
    "# Select the specific data point at index 100\n",
    "index = 100\n",
    "data_point = test_data[index]\n",
    "\n",
    "predicted_label = test_predictions[index]\n",
    "actual_label = test_label[index]\n",
    "\n",
    "print(f\"Actual Label at index {index}: {actual_label}\")\n",
    "print(f\"Predicted Label at index {index}: {predicted_label}\")\n",
    "\n",
    "# Visualize the data point\n",
    "if len(data_point) == 784:  # Assuming it's a 28x28 image flattened\n",
    "    image = data_point.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f'Actual Label: {actual_label}, Predicted Label: {predicted_label}')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Data point is not an image, visualizing as a line plot.\")\n",
    "    plt.plot(data_point)\n",
    "    plt.title(f'Actual Label: {actual_label}, Predicted Label: {predicted_label}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
